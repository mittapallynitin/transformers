{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMB_SIZE = 128\n",
    "MAX_SEQ_LEN = 512\n",
    "FEED_FORWARD_EXPANSION = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int):\n",
    "        \"\"\"Initializes the InputEmbeddings module.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "            emb_size (int): The size of each embedding vector.\n",
    "\n",
    "        Attributes:\n",
    "            embedding (nn.Embedding): Embedding layer that maps input tokens to their embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) / math.sqrt(self.emb_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size, max_len=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the PositionalEncoding module. Uses the sinusoidal encoding scheme.\n",
    "\n",
    "        Args:\n",
    "            emb_size (int): The size of each embedding vector.\n",
    "            dropout (float): The dropout rate.\n",
    "            max_len (int, optional): The maximum length of the input sequence.\n",
    "                Defaults to 5000.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Create a tensor for position indices # Shape: [max_len, 1]\n",
    "        positions = torch.arange(0, self.max_len).unsqueeze(1)\n",
    "\n",
    "        # Create a tensor for the even indices\n",
    "        div_term = 10000 ** (torch.arange(0, self.emb_size, 2).float() / self.emb_size)\n",
    "\n",
    "        # Apply sine and cosine functions on the entire tensor in one go\n",
    "        pe = torch.zeros(self.max_len, self.emb_size)\n",
    "\n",
    "        # Sine for even indices\n",
    "        pe[:, 0::2] = torch.sin(positions / div_term)\n",
    "\n",
    "        # Cosine for odd indices\n",
    "        pe[:, 1::2] = torch.cos(positions / div_term)\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)  # Shape: [1, max_len, emb_size]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.shape[1]]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 14]), 4, 14)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    \"This is sample sentence one\",\n",
    "    \"This is sample sentence two\", \n",
    "    \"This is a very large sample sentence to increase the sequence length\", \n",
    "    \"This is small\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "B, L = tokens.input_ids.shape\n",
    "tokens.input_ids.shape, B, L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2023, 2003, 7099, 6251, 2028,  102,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 101, 2023, 2003, 7099, 6251, 2048,  102,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 101, 2023, 2003, 1037, 2200, 2312, 7099, 6251, 2000, 3623, 1996, 5537,\n",
       "         3091,  102],\n",
       "        [ 101, 2023, 2003, 2235,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = InputEmbeddings(VOCAB_SIZE, EMB_SIZE)\n",
    "positional_encoding = PositionalEncoding(EMB_SIZE, max_len=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings: torch.Size([4, 14, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14, 128])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = input_embeddings(tokens[\"input_ids\"])\n",
    "print(\"Input Embeddings:\", x.shape)\n",
    "x = positional_encoding(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 128])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding.pe[:, :14, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class selfAttentionHead(nn.Module):\n",
    "    def __init__(self, emb_size, head_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(emb_size, head_dim, bias=False)\n",
    "        self.key = nn.Linear(emb_size, head_dim, bias=False)\n",
    "        self.value = nn.Linear(emb_size, head_dim, bias=False)\n",
    "        self.scale = torch.Tensor([emb_size ** -0.5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        scores  = torch.bmm(query, key.transpose(1, 2)) * self.scale\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(attention_weights, value)\n",
    "        return context\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, n_heads):\n",
    "        super().__init__()\n",
    "        assert emb_size % n_heads == 0\n",
    "        self.head_dim = emb_size // n_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [selfAttentionHead(emb_size, self.head_dim) for _ in range(n_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(emb_size, emb_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.linear(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_block = selfAttentionHead(EMB_SIZE, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = attention_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attention_bloc = MultiHeadAttention(EMB_SIZE, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14, 16])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = multihead_attention_bloc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_size, expansion_factor):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.expansion_factor = expansion_factor\n",
    "        \n",
    "        self.linear1 = nn.Linear(emb_size, expansion_factor * emb_size)\n",
    "        self.linear2 = nn.Linear(expansion_factor * emb_size, emb_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward = FeedForward(EMB_SIZE, FEED_FORWARD_EXPANSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14, 128])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward(x).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
